{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center>Brazilian traffic sign recognition: an neural network approach</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Besides autonomous cars technology is actually a trending subject in researches, many subfuctions of a car's embedded system has already been developed and it's currently working in nowadays cars (maybe not in popular ones). Traffic signs recognition (TSR) is one of those already done functions and it has a good performance, but as a driver assistant not as a controller product. Additionally, the recognition system is usually created based in the manufacture country regulations and features. As basically all cars sold in Brazil has imported technology, it's recognition function is disconnected from the system as it has no use there.\n",
    "\n",
    "This paper propose the creation of a dataset based in Brazil's traffic signs regulation, as well a artificial intelligence (AI) application based in training a neural network to classify it. The AI was implemented in TensorFlow framework for Python 3.7.\n",
    "\n",
    "\n",
    "![Examples of brazilian's Traffic signs](http://www.betuseal.com.br/wp-content/uploads/2015/01/placas-transito-sinaliza%C3%A7%C3%A3o-675x414.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Libraries used:\n",
    "\n",
    "- Matplotlib: Python 2D plotting library;\n",
    "- NumPY: Python math package to work with arrays, vectors and matrices;\n",
    "- OS: File and Directory Access;\n",
    "- OpenCV: Image data processing;\n",
    "- Sklearn: Data mining and data analysis;\n",
    "- Skimage: Image data processing;\n",
    "- TensorFlow: Machine Learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiago.daros/anaconda2/envs/idp2018/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.transform import rotate\n",
    "from skimage.transform import warp\n",
    "from skimage.transform import ProjectiveTransform\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from random import choice\n",
    "from random import randint\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the dataset\n",
    "\n",
    "Examples of Brazilian traffic signs can be individually gathered in [Detran/SE](http://www.detran.se.gov.br/educ_sinal.asp) web site, although it is a manual task, it wouldn't take so long to finish.\n",
    "\n",
    "<table><tr><td><img width=100 src='traffic_signs_classes/0.png'></td><td><img width=100 src='traffic_signs_classes/6.png'></td><td><img width=100 src='traffic_signs_classes/12.png'></td><td><img width=100 src='traffic_signs_classes/13.png'></td></tr></table>\n",
    "\n",
    "As traffic sign has a default shape, it allows removing any unnecessary part and retaining only the important details, reducing the number of data and so, optimizing for data processing.\n",
    "\n",
    "\n",
    "There are differents borders and shapes of signs, as illustrated above, which indicates it's objective (warning, regulation and directions) but it doens't offer any precious information about the sign meaning, so as the colors used. Such statement allow us to reduce data number  without losing any important feature for recognition by previously removing the sign background and convert it to gray scale.\n",
    "\n",
    "Due to remove the signs background, It was manually created a image for each sign in directory \"thresholding\", which contain only the background in the shape of the corresponding sign, also this image has the same name as the original sign for implementation facilities. The examples of thresholding images for the shown signs are demonstrated below.\n",
    "\n",
    "<table><tr><td><img width=100 src='traffic_signs_classes/thresholding/0.jpg'></td><td><img width=100 src='traffic_signs_classes/thresholding/6.jpg'></td><td><img width=100 src='traffic_signs_classes/thresholding/12.jpg'></td><td><img width=100 src='traffic_signs_classes/thresholding/13.jpg'></td></tr></table>\n",
    "\n",
    "Thinking in a real world application, the sign wouldn't be in perfect state and possibly in a different perspective as it should be, so it would be necessary to create those conditions to relate this possibilities to a label. From the 30 signs gathered in the previous database, a total of 500 synthesized versions for each sign were created. Each one of these versions proposes random geometry distortions, brightness, contrast, blur and noises effects, and at least, it is saved as a 56x56 gray pixels gray scale image, as its ilustraded below.\n",
    "\n",
    "<table><tr><td><img width=100 src='Generated/0/0_1.png'></td><td><img width=100 src='Generated/6/6_7.png'></td><td><img width=100 src='Generated/12/12_13.png'></td><td><img width=100 src='Generated/13/13_14.png'></td></tr></table>\n",
    "\n",
    "After all the image processing, our Brazilian traffic sign image dataset is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 29 Image : 0 Transformation : 0\n",
      "Class: 29 Image : 0 Transformation : 1\n",
      "Class: 29 Image : 0 Transformation : 2\n",
      "Class: 29 Image : 0 Transformation : 3\n",
      "Class: 29 Image : 0 Transformation : 4\n",
      "Class: 29 Image : 0 Transformation : 5\n",
      "Class: 29 Image : 0 Transformation : 6\n",
      "Class: 29 Image : 0 Transformation : 7\n",
      "Class: 29 Image : 0 Transformation : 8\n",
      "Class: 29 Image : 0 Transformation : 9\n",
      "Class: 29 Image : 0 Transformation : 10\n",
      "Class: 29 Image : 0 Transformation : 11\n",
      "Class: 29 Image : 0 Transformation : 12\n",
      "Class: 29 Image : 0 Transformation : 13\n",
      "Class: 29 Image : 0 Transformation : 14\n",
      "Class: 29 Image : 0 Transformation : 15\n",
      "Class: 29 Image : 0 Transformation : 16\n",
      "Class: 29 Image : 0 Transformation : 17\n",
      "Class: 29 Image : 0 Transformation : 18\n",
      "Class: 29 Image : 0 Transformation : 19\n"
     ]
    }
   ],
   "source": [
    "def pespective(input, mascara):\n",
    "    cols = input.shape[1]\n",
    "    rows = input.shape[0]\n",
    "    inputQuad = np.array([[0, 0],[cols, 0],[cols,rows],[0,rows]], np.float32)\n",
    "    outputQuad = np.array([[-randint(0,17),-randint(0,17)],[cols+randint(0,17),-randint(0,17)],\t\n",
    "\t[cols+randint(0,17), rows+randint(0,17)],[-randint(0,17),rows+randint(0,17)]], np.float32)\n",
    "    M = cv2.getPerspectiveTransform(outputQuad, inputQuad)\n",
    "    output = cv2.warpPerspective(input,M,(cols,rows)) \n",
    "    output2 = cv2.warpPerspective(mascara,M,(cols,rows)) \n",
    "    return [output,output2]\n",
    "\n",
    "\n",
    "'''\n",
    "Apply 2D rotation and perspective transformation\n",
    "'''\n",
    "\n",
    "def transformation(name, name2):\n",
    "    src1 = cv2.imread(name, 0)\n",
    "    mask = cv2.imread(name2, 0)\n",
    "    \n",
    "    '''\n",
    "    noise application\n",
    "    '''\n",
    "    \n",
    "    rows, cols = src1.shape\n",
    "    k = 0\n",
    "    for i in range(0, rows):\n",
    "        for j in range(0, cols):\n",
    "            editValue = src1[i, j]\n",
    "            var = randint(25, 100)\n",
    "            k = var\n",
    "            if editValue > 60 and editValue < 160:\n",
    "                src1[i, j] = var\n",
    "            elif editValue > 230 and editValue < 256:\n",
    "                src1[i, j] = 255 - var\n",
    "    if k > 40:\n",
    "        getElement = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        src1 = cv2.erode(src1, getElement)\n",
    "\n",
    "        getElement = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        src1 = cv2.dilate(src1, getElement)\n",
    "\n",
    "    '''\n",
    "    rotation transformation (-20 ate 20 graus)\n",
    "    '''\n",
    "    \n",
    "    image_center = tuple(np.array(src1.shape) / 2)\n",
    "    angle = randint(-30, 30)\n",
    "    rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
    "    result = cv2.warpAffine(src1, rot_mat, src1.shape, flags=cv2.INTER_LINEAR)\n",
    "    result2 = cv2.warpAffine(mask, rot_mat, mask.shape, flags=cv2.INTER_LINEAR)     \n",
    "\n",
    "\n",
    "    '''\n",
    "    perspective transformation\n",
    "    '''\n",
    "    \n",
    "    img = result\n",
    "    rows = img.shape[0]\n",
    "    cols = img.shape[1]\n",
    "    pts1 = np.float32([[0, 0], [0, 200], [200, 0], [200, 200]])\n",
    "    seed = randint(0, 1)\n",
    "    variation1 = randint(0, 8)\n",
    "    variation2 = randint(15, 25)\n",
    "\n",
    "    # right direction perspective\n",
    "    if seed == 0:\n",
    "        pts2 = np.float32([[(0 + variation1), 0], [(0 + variation1),\n",
    "                                                   200], [(200 - variation2), 0], [(200 - variation2), 200]])\n",
    "    # left direction perspective\n",
    "    else:\n",
    "        pts2 = np.float32([[(0 + variation2), 0], [(0 + variation2),\n",
    "                                                   200], [(200 - variation1), 0], [(200 - variation1), 200]])\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    dst = cv2.warpPerspective(img, M, (200, 200))\n",
    "    dst2 = cv2.warpPerspective(result2, M, (200, 200))\n",
    "    \n",
    "    return [dst, dst2]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Save resized image after applying random brightness, constrast and blur effect\n",
    "'''\n",
    "\n",
    "def variation(image,mascara, pasta, arquivo, transf):\n",
    "    \n",
    "    # alpha random values (simple contrast control) (0.4 - 0.9)\n",
    "    alpha = uniform(.4, .9)\n",
    "\n",
    "    # beta random values (simple brightness control) (10 - 20)\n",
    "    beta = randint(12, 16)\n",
    "    \n",
    "    cv2.convertScaleAbs(image, image, alpha, beta)\n",
    "\n",
    "    # add gaussian blurring\n",
    "    size = int(choice('35'))\n",
    "    blur = cv2.GaussianBlur(image, (size, size), 0, 0)\n",
    "    uhu =  cv2.medianBlur(blur,5)    \n",
    "    th, mask_th = cv2.threshold(mascara, 50, 255, cv2.THRESH_BINARY);\n",
    "    blur = cv2.bitwise_and(uhu, mask_th)\n",
    "    final = cv2.resize(blur, (56, 56))\n",
    "  \n",
    "    \n",
    "    cv2.imwrite(\"Generated/\" + str(pasta)+ \"/\"+ str(arquivo) + \"_\" +str(transf) + \".png\", final)\n",
    "\n",
    "    \n",
    "'''\n",
    "Import each image and pass it data as arguments to the image processing functions\n",
    "In the end of it's execution, all the synthesized images will be saved in \"Generated\" directory\n",
    "'''\n",
    "\n",
    "def create_database():\n",
    "    flag_name = 0\n",
    "    for i in range(29, 30):\n",
    "        filepath = './traffic_signs_classes/' + str(i) + '.png'\n",
    "        filepath2 = './traffic_signs_classes/thresholding/' + str(i) + '.jpg'\n",
    "        for j in range(0, 25):\n",
    "            src, mask = transformation(filepath, filepath2)\n",
    "            for transf in range(0, 20):\n",
    "                src2, mask2 = pespective(src, mask)\n",
    "                variation(src2, mask, i, j, transf)\n",
    "                print('Class: ' + str(i) + ' Image : ' + str(j) + ' Transformation : ' + str(transf))\n",
    "    print('Dataset successfully created!')\n",
    "\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "create_database()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that a diversified traffic sign samples were created, it's ready to be loaded and pre-processed before initialize the neural network.\n",
    "\n",
    "As usual for image classification, it's matrix element becomes an feature for classification, so a 56 x 56 x 1 (length, width, channel) image is converted to a 56x56x1 = 3136 elements array. Each dataset matrix row correspond to a single sign image loaded, therefore our dataset matrix size is 15000 rows and 3136 columns. \n",
    "\n",
    "As for the label matrix, which relate the feature row to it's traffic sign, the concept of one hot encoding is used. Then, a total of 30 columns are needed for this classification, which each column represents one of the signs examples included in dataset. The number of rows is equal for both feature and label matrix.\n",
    "\n",
    "For the importing algorithm, it will enter each directory where was generated the different samples of each sign, load it's image data in gray scale and append it in the pre-initialized matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated_filepath = os.getcwd() + '/Generated//'\n",
    "n_signs = 30  # -1 as matrix index stars at 0\n",
    "y_matrix = np.empty([0, n_signs])\n",
    "x_matrix = []\n",
    "for sign_count in range(0,n_signs):\n",
    "        inputdir =  generated_filepath + str(sign_count) + '//'\n",
    "        y_sign = np.zeros((1,n_signs), dtype=np.int)\n",
    "        y_sign[0][sign_count] = 1\n",
    "        for image in os.listdir(inputdir):\n",
    "            imageFilepath = inputdir + image\n",
    "            img_matrix = cv2.imread(imageFilepath,0)\n",
    "            img_array = np.ravel(img_matrix)\n",
    "            y_matrix = np.concatenate((y_matrix, y_sign), axis=0)\n",
    "            x_matrix.append(img_array)\n",
    "x_matrix = np.array(x_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network\n",
    "\n",
    "In AI subject, neural network concept was chosen for it's proven efficiency with image classification. As a first approach, it was chosen to use a multi-layer perceptron (MLP) network.\n",
    "\n",
    "### 3.1 Multi-Layer Perceptron\n",
    "\n",
    "Perceptron is an algorithm that maps a function by binary classification. A single layer perceptron example is shown below.\n",
    "\n",
    "![Perceptron function](images/perceptron.png)\n",
    "\n",
    "A simplified perceptron algorithm follow these steps:\n",
    "\n",
    "1. Initialize the weights with random values;\n",
    "2. Select an input vector and present it to the network;\n",
    "3. Compute the output y', the input vectors ($x_i$) and the weights values ($\\omega_i$);\n",
    "4. Apply binary function:\n",
    "\n",
    "$$f(x) =\\begin{cases}\n",
    "               1\\hspace{1mm}if\\hspace{1mm}\\omega\\hspace{1mm}\\times\\hspace{1mm}x\\hspace{1mm}+\\hspace{1mm}b\\hspace{1mm}>\\hspace{1mm}0\\\\\n",
    "               0\\hspace{1mm}if\\hspace{1mm}otherwise\n",
    "            \\end{cases}$$\n",
    "\n",
    "5. If y' $\\neq$ y modify all connections $\\omega_i$ by adding the changes $\\bigtriangleup\\omega = yx_i$;\n",
    "6. Return to step 2.\n",
    "\n",
    "The single layer perceptron can be generalized to many layers connected to each other, as illustrated below.\n",
    "\n",
    "![Multi Layer Perceptron](images/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementation\n",
    "\n",
    "The MLP network is implemented though TensorFlow framework, which includes several functions for Machine Learning. In addition, the environment has Intel Distribution for Python package to enhance the application performance\n",
    "\n",
    "#### 3.2.1 Model Specification\n",
    "\n",
    "Before instantiate the hidden layers, it's necessary to do three things:\n",
    "\n",
    "1. Define the input and output of the data;\n",
    "2. Choose number of hidden layers and number of it's inputs;\n",
    "3. Create the variables corresponding to each weights and bias needed for this neural network.\n",
    "\n",
    "Through Tensorflow method tf.placeholder, input and output is instantiate in the data type and length specified in it's arguments.\n",
    "\n",
    "For a first attempt, it was chosen to use 2 hidden layers with 300 inputs each, so it needs 3 vectors for each variable (weight and bias), as it includes output layer as well. As the parameters are specified, it should initialize it with help of tf.Variable and a value in it's argument. This value can be zero, one or random.\n",
    "\n",
    "Now that all the variables are defined, a layer is created associating those parameters with tf.add and tf.matmul. After it's building, a activation function (in this case, relu or softmax) is associated to each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = np.shape(x_matrix)[1]\n",
    "y_output = np.shape(y_matrix)[1]\n",
    "x = tf.placeholder(tf.float32,[None,n_input])\n",
    "y = tf.placeholder(tf.float32,[None,y_output])\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([n_input, n_hidden_1]))\n",
    "w2 = tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2]))\n",
    "b1 = tf.Variable(tf.random_normal([n_hidden_1]))\n",
    "b2 = tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "b_out = tf.Variable(tf.random_normal([y_output]))\n",
    "w_out = tf.Variable(tf.random_normal([n_hidden_2, y_output]))\n",
    "\n",
    "layer_1 = tf.add(tf.matmul(x, w1), b1)\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "layer_2 = tf.add(tf.matmul(layer_1, w2), b2)\n",
    "layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "out_layer = tf.add(tf.matmul(layer_2,w_out),b_out)\n",
    "pred = tf.nn.softmax(out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the layers are already done, only the optimizer and cost functions are still missing. Cost will be defined by Cross-Entropy method, which calculate the difference value between the actual and the expected output with help of tf.reduce_mean Tensorflow method.\n",
    " \n",
    "For optimizer, Adam gradient descent function was selected as it performs smooth updates in calculating a new weight and bias in function of the cost retrieved for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_layer,labels=y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model definition is completed and it's ready for training, but it lacks a measure to demonstrate effectiveness. So, as a matter of accuracy calculation between epochs and display it for user visualization, every epoch will run and store the result of an algorithm that compares the maximum classification from output vector and the desired output for the sample injected in the network. If both are equal, it represents a right classification from the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = tf.equal(tf.argmax(out_layer,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,'float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Pre-processing data\n",
    "\n",
    "Before splitting the dataset matrix in a training set and validation set for cross-validation method application, it's applied a feature scaling so it's values becomes between 0~1, to offer at least a faster convergence in training phase. It's done with help from sklearn preprocessing method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiago.daros/anaconda2/envs/idp2018/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype uint8 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_matrix_scale = min_max_scaler.fit_transform(x_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Execution of training\n",
    "\n",
    "The dataset is separated in the session's beginning in to test set and validation set, as it will be used for cross-validation method. The test set length is defined as 33% of total dataset length, and then it will contain 4950 sign samples, as validation set will have 10050 samples. This splitting number is very usual, as it's a good practice to have a test set size lesser than the validation set size to avoid overfitting.\n",
    "\n",
    "The number of epochs running this training is defined above and it's defined equal 150 to provide enough information in how many epochs are needed to have a good classification rate without overtraining this neural network. Each epoch run will call the optimizer and cost functions, to acquire the networks weights and bias vectors, and after it will run the manual accuracy method that was told before to store and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhJQ/x8UTWdUAAAABElEQVSJtNn7wrAduAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe38c82d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.313130378723145\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    b = []\n",
    "    ac = []\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x_matrix_scale, y_matrix, test_size=0.33)\n",
    "    for i in range(epochs):\n",
    "        _, a = sess.run([optimizer,cost],feed_dict={x:X_train,y:Y_train})\n",
    "        c = sess.run(accuracy,feed_dict={x:X_test,y:Y_test})\n",
    "        b.append(a)\n",
    "        ac.append(c)\n",
    "    \n",
    "    plt.plot(b)\n",
    "    plt.show()\n",
    "    \n",
    "    print(100*max(ac))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Results Analysis\n",
    "\n",
    "The MLP neural network results nearly 70% of accuracy, which represents that from 10050 samples used for validation, almost 7035 were right classified and 3015 were wrong. The resuls may change due the train_test_split method uses random set generation, so the test and validation set are probably different for each call. \n",
    "\n",
    "Thinking in a real world application in autonomous cars, it would not be a successful solution as it permits many recognition mistakes and promotes a bad decision, putting lives in risk. As a first attempt for this problem, it is a good result, but it also needs a better approach as desired for future works in this subject, like application of different neural network concepts as convolutional neural network for example, and optimization of parameters as layers and number of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional Neural Networks (CNN)\n",
    "\n",
    "When it comes to computer vision field, convolutional neural networks are well-known for performing nicely in object classification problems. They are biological inspired in order to handle a minimum pre-processing. Additionally, convolutional neural networks uses the same principle of weights and biases of MLP, although it works in a more locally way, preserving the spatial relationship between near pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1 Implementation\n",
    "\n",
    "Once again, the TensorFlow framework is used with Intel Distribution for Python package to enhance the application performance. \n",
    "\n",
    "This implementation uses two sets of convolutional and pooling layers. Note that this can be repeated multiple times, in fact, modern high performance CNNs have many alternating layers. On this work, a very common topology is used to demonstrate the superiority over MLPs in an arbitrary context.\n",
    "\n",
    "![CNN](images/cnn.png)\n",
    "\n",
    "The figure above shows the used convolutional neural network simplified diagram. Pooling and activation functions are ommited. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Define Proprieties\n",
    "\n",
    "To the better understanding of the tensor dimensions, it is nice to define clearly the images proprieties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The image size, assuming the image is a square (a x a)\n",
    "img_size = 56\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the layer parameters and batch size are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 16         # There are 32 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 36         # There are 64 of these filters.\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer.\n",
    "\n",
    "batch_size = 128          # Batch size defined to further iterate in batches and thus avoid memory issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Dataset Arrangement\n",
    "\n",
    "The next function 'train_test_split' is a `sklearn` method for splitting the dataset into test and train parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(np.float32(x_matrix_scale), np.float32(y_matrix), test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11600 images in the train set, thus batches are used to improve the performance and prevent crashing due to RAM issues. The next methods are imported from TensorFlow and they are responsible to create a new batch every time an iteration is completed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()  #Clears the graph stack\n",
    "\n",
    "# Batch acquiring methods\n",
    "tr_data = tf.contrib.data.Dataset.from_tensor_slices((X_train,Y_train))\n",
    "batched_train = tf.contrib.data.Dataset.batch(tr_data, batch_size=batch_size)\n",
    "iterator = tf.contrib.data.Iterator.from_structure(batched_train.output_types,batched_train.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "train_init = iterator.make_initializer(batched_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.1.3 Model Building\n",
    "\n",
    "The first step, as aforementioned, is to define placeholders and initialize input vectors through tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Placeholder variable for the input images.\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x') \n",
    "#Reshape the input image as a 4 dimension tensor. '-1' is used to infer the shape.\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "#Placeholder variable for the labels.\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "#Returns the index with the largest value\n",
    "y_true_cls = tf.argmax(y_true, axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start describing the convolutional network structure. Layer 1 is defined by an arbitrary number of filters (`num_filters1`) with size `filter_size1`. Another parameter defined is the stride, which determines the filter step size over the input image. Here, the stride is selected as `1` on the x-axis and `1` on the  y-axis.\n",
    "\n",
    "The weights and biases are created with random values.\n",
    "\n",
    "A max-pooling layer is used to halve the image size by a scale factor of `2`. Lastly, a ReLU operator is used to add non-linearity to the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convolutional Layer 1\n",
    "w1 = tf.Variable(tf.truncated_normal([filter_size1, filter_size1, num_channels, num_filters1], stddev=0.05))\n",
    "b1 = tf.Variable(tf.constant(0.05, shape=[num_filters1]))\n",
    "lc1 = tf.nn.conv2d(input=x_image, filter=w1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "lc1 += b1\n",
    "lc1 = tf.nn.max_pool(value=lc1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding='SAME')\n",
    "lc1 = tf.nn.relu(lc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layer `2` is defined the same way as layer `1`, except its input is the output of layer `1` (`lc1`) and the size and number of filters are defined by `fiter_size2` and `num_filters2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convolutional Layer 2\n",
    "w2 = tf.Variable(tf.truncated_normal([filter_size2, filter_size2, num_filters1, num_filters2], stddev=0.05))\n",
    "b2 = tf.Variable(tf.constant(0.05, shape=[num_filters2]))\n",
    "lc2 = tf.nn.conv2d(input=lc1, filter=w2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "lc2 += b2\n",
    "lc2 = tf.nn.max_pool(value=lc2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "lc2 = tf.nn.relu(lc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fully Connected Layer requires a vector as input, so function `reshape` is used to build a flat layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "\n",
    "layer_shape = lc2.get_shape()\n",
    "num_features = layer_shape[1:4].num_elements()\n",
    "lf = tf.reshape(lc2, [-1, num_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layer 1 size is determined by `fc_size`. Here, we fall in the same context of a Muilti Layer Perceptrion network. A ReLU functio is used as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fully connected layer 1\n",
    "\n",
    "wfc1 = tf.Variable(tf.truncated_normal([num_features, fc_size], stddev=0.05))\n",
    "bfc1 = tf.Variable(tf.constant(0.05, shape=[fc_size])) \n",
    "fc1 = tf.matmul(lf, wfc1) + bfc1\n",
    "fc1 = tf.nn.relu(fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layer 2 inputs are determined by the output size of Fully Connected Layer 1. The desired output is the number of classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fully connected layer 2\n",
    "\n",
    "wfc1 = tf.Variable(tf.truncated_normal([fc_size, num_classes], stddev=0.05))\n",
    "bfc1 = tf.Variable(tf.constant(0.05, shape=[num_classes])) \n",
    "fc2 = tf.matmul(fc1, wfc1) + bfc1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Softmax function is used as classifier. Cross entropy and optimizer are defined with the same options as in MLP network. Also, accuracy and cost functions are measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class prediction\n",
    "y_pred = tf.nn.softmax(fc2)\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc2,\n",
    "                                                        labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=20e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Training\n",
    "\n",
    "The next section execute iterations to run the optimizator in order to reduce the cost function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Iteration:      1, Loss: 3.41)\n",
      "Iteration:     17, Loss: 3.07)\n",
      "Iteration:     33, Loss: 1.99)\n",
      "Iteration:     49, Loss: 1.21)\n",
      "Iteration:     65, Loss: 0.72)\n",
      "Time usage: 0:00:16\n",
      "accuracy: 88.34%\n",
      "Epoch: 2\n",
      "Iteration:      1, Loss: 0.41)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "epochs = 10\n",
    "with tf.Session() as sess:\n",
    "    sess.run([train_init, tf.global_variables_initializer()])\n",
    "    #saver.restore(sess, \"./cnn_save1\")\n",
    "    total_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        start_time = time.time()\n",
    "        print ('Epoch: {}'.format(i+1))\n",
    "        j=0\n",
    "        while(True):\n",
    "            try:\n",
    "                a,b = sess.run(next_element)\n",
    "            except:\n",
    "                sess.run(train_init)\n",
    "                break\n",
    "                   \n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict={x:a, y_true:b})\n",
    "            \n",
    "            if j % 16 == 0: # Print Loss every 4 iterations.\n",
    "                msg = \"Iteration: {0:>6}, Loss: {1:.2f})\"\n",
    "                print(msg.format(j + 1, loss))\n",
    "\n",
    "            j+=1\n",
    "        \n",
    "        # Time estimative.\n",
    "        end_time = time.time()\n",
    "        time_dif = end_time - start_time\n",
    "        \n",
    "        print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "        \n",
    "        # Accuracy calculation.\n",
    "        acc = sess.run(accuracy, feed_dict={x: X_test, y_true: Y_test})\n",
    "        print (\"accuracy: {0:.2f}%\".format(100*acc))\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - total_time\n",
    "    print(\"Total time: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "    saver.save(sess, \"./cnn_save1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.5 Results Analysis\n",
    "\n",
    "The Convolutional Neural Network achieves about 99.5% of accuracy with just some minutes of training. This result shows how the use of neural networks in everyday tasks with a good choice of topology can become feasible, even in critical tasks as autonomous vehicles. \n",
    "\n",
    "[todo] Time comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
